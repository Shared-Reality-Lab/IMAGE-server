#!/bin/bash

SERVER_NAME=$(hostname)

TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

SCRIPT_DIR="$(dirname "$(realpath "$0")")"
ENV_FILE="$SCRIPT_DIR/../config/healthcheck.env"

if [ -f "$ENV_FILE" ]; then
  export $(grep -v '^#' "$ENV_FILE" | xargs)
else
  echo "Error: Environment file not found at $ENV_FILE" >&2
  exit 1
fi

LOG_FILE="${LOG_DIR}/docker_healthcheck_${TIMESTAMP}.log"

touch "$LOG_FILE"
if [ ! -w "$LOG_FILE" ]; then
  echo "Error: Log file is not writable. Please check permissions." >&2
  exit 1
fi

# Get all containers attached to the image network, including stopped ones
ALL_IMAGE_CONTAINERS=$(docker ps -a --filter "network=image" --format '{{.Names}}')
echo "ALL IMAGE CONTAINERS ON $SERVER_NAME: $ALL_IMAGE_CONTAINERS" >> "$LOG_FILE"

# Get all currently running container IDs and names on the image network
RUNNING_CONTAINERS=$(docker ps --filter "network=image" --format "{{.ID}} {{.Names}}")
echo "ALL RUNNING CONTAINERS ON $SERVER_NAME: $RUNNING_CONTAINERS" >> "$LOG_FILE"

MESSAGES=""

# --- HOST GPU HEALTH (NVML + driver version mismatch) ------------------------
# Host-side GPU check:
# - run nvidia-smi (NVML) with a short timeout
# - compare driver version from nvidia-smi vs kernel module

SMI_BIN="$(command -v nvidia-smi || true)"
TIMEOUT_SEC=5

# 1) NVML / nvidia-smi availability
if [ -z "$SMI_BIN" ] || ! timeout "${TIMEOUT_SEC}s" "$SMI_BIN" >/dev/null 2>&1; then
  MSG=":warning: Host *$SERVER_NAME* GPU *unhealthy*: NVML unavailable (nvidia-smi failed)."
  MESSAGES+="$MSG"$'\n'
  echo "$(date) - $MSG" >> "$LOG_FILE"
else
  # 2) Compare numeric driver versions from nvidia-smi vs kernel module
  SMI_VER="$(timeout "${TIMEOUT_SEC}s" "$SMI_BIN" --query-gpu=driver_version --format=csv,noheader 2>/dev/null \
            | awk 'NF' | sort -u | head -n1)"

  # parse kernel module's version from /proc
  PROC_VER="$(sed -nE 's/.*Kernel Module[[:space:]]+([0-9]+\.[0-9]+\.[0-9]+).*/\1/p' /proc/driver/nvidia/version 2>/dev/null | head -n1)"

  if [ -z "$SMI_VER" ] || [ -z "$PROC_VER" ]; then
    MSG=":warning: Host *$SERVER_NAME* GPU *warning*: could not read driver versions (smi='${SMI_VER:-NA}', proc='${PROC_VER:-NA}')."
    MESSAGES+="$MSG"$'\n'
    echo "$(date) - $MSG" >> "$LOG_FILE"
  elif [ "$SMI_VER" != "$PROC_VER" ]; then
    # grab CUDA version from the nvidia-smi header
    SMI_HEAD="$(timeout "${TIMEOUT_SEC}s" "$SMI_BIN" | head -n1 2>/dev/null)"
    CUDA_VER="$(echo "$SMI_HEAD" | sed -n 's/.*CUDA Version: *\([0-9.]\+\).*/\1/p')"

    MSG=":warning: Host *$SERVER_NAME* – NVIDIA driver *mismatch* detected:
• nvidia-smi: ${SMI_VER}
• kernel:     ${PROC_VER}"
    if [ -n "$CUDA_VER" ]; then
      MSG="$MSG"$'\n'"• CUDA:       ${CUDA_VER}"
    fi
    MSG="$MSG"$'\n'"Consider scheduling a reboot to reconcile."

    MESSAGES+="$MSG"$'\n'
    echo "$(date) - NVIDIA driver mismatch on $SERVER_NAME (smi=${SMI_VER}, kernel=${PROC_VER}${CUDA_VER:+, cuda=${CUDA_VER}})" >> "$LOG_FILE"
  else
    # versions match — log a simple OK line (no Slack)
    SMI_HEAD="$(timeout "${TIMEOUT_SEC}s" "$SMI_BIN" | head -n1 2>/dev/null)"
    CUDA_VER="$(echo "$SMI_HEAD" | sed -n 's/.*CUDA Version: *\([0-9.]\+\).*/\1/p')"
    echo "$(date) - Host GPU healthy (driver=${SMI_VER}${CUDA_VER:+, cuda=${CUDA_VER}})." >> "$LOG_FILE"
  fi
fi

# Loop through the list of all containers on the "image" network
for EXPECTED_CONTAINER in $ALL_IMAGE_CONTAINERS; do
  FOUND=0

  # Checks if an expected container is running, and if so, whether it is unhealthy.
  while read -r CONTAINER_ID CONTAINER_NAME; do
    if [ "$CONTAINER_NAME" == "$EXPECTED_CONTAINER" ]; then
      FOUND=1
      # Check health status if the container is running
      STATUS=$(docker inspect --format='{{if .State.Health}}{{.State.Health.Status}}{{else}}not-applicable{{end}}' $CONTAINER_ID 2>>"$LOG_FILE")
      if [ "$?" -ne 0 ]; then
        echo "$(date) - Error inspecting container $CONTAINER_NAME (ID: $CONTAINER_ID)" >> "$LOG_FILE"
      elif [ "$STATUS" == "unhealthy" ]; then
        MESSAGE=":warning: Container *$SERVER_NAME - $CONTAINER_NAME* (ID: $CONTAINER_ID) is *unhealthy*."
        MESSAGES+="$MESSAGE"$'\n'
        echo "$(date) - $MESSAGE" >> "$LOG_FILE"
      elif [ "$STATUS" == "not-applicable" ]; then
        echo "$(date) - Container $CONTAINER_NAME on $SERVER_NAME (ID: $CONTAINER_ID) does not have a health check." >> "$LOG_FILE"
      else
        echo "$(date) - Container $CONTAINER_NAME (ID: $CONTAINER_ID) is healthy." >> "$LOG_FILE"
      fi
    fi
  done <<< "$RUNNING_CONTAINERS"

  # Checks if any of the containers are not running at all. If not, it is appended to messages, that will later be sent to Slack.
  if [ "$FOUND" -eq 0 ]; then
    MESSAGE=":warning: Expected container *$EXPECTED_CONTAINER on $SERVER_NAME* is not running."
    MESSAGES+="$MESSAGE"$'\n'
    echo "$(date) - $MESSAGE" >> "$LOG_FILE"
  fi
done

# Send collected messages to Slack if there are any
if [ -n "$MESSAGES" ]; then
  # Ensure SLACK_WEBHOOK_URL is set and not empty
  if [ -z "$SLACK_WEBHOOK_URL" ]; then
    echo "$(date) - SLACK_WEBHOOK_URL is not set. Cannot send notifications." >> "$LOG_FILE"
  else
    RESPONSE=$(curl -X POST -H 'Content-type: application/json' --data "{\"text\": \"$MESSAGES\"}" "$SLACK_WEBHOOK_URL" 2>>"$LOG_FILE")
    if [ "$?" -ne 0 ]; then
      echo "$(date) - Error sending Slack notification. Curl command failed!" >> "$LOG_FILE"
    else
      echo "$(date) - Slack notification sent successfully." >> "$LOG_FILE"
    fi
  fi
else
  echo "$(date) - No issues detected. No notifications sent." >> "$LOG_FILE"
fi

